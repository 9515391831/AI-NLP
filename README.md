<h1><center>NATURAL LANGUAGE PROCESSING</center></h1>

Natural Language Processing is the technology used to aid computers to
understand the human’s natural language like English. The objective of NLP
is to read, understand, and make sense of the human languages.
NLP techniques rely on machine learning to derive meaning from human
languages.

<img src='https://www.experfy.com/blog/wp-content/uploads/2018/07/medium_2fd8ce3d-4ab8-40c4-96dd-9792725503a6.png'>

<img src='https://data-flair.training/blogs/wp-content/uploads/sites/2/2018/08/NLTK-NLP-with-Python.jpg'>

<img src='https://www.contrib.andrew.cmu.edu/~dyafei/images/NLP01.jpg'>

<h3>Tokenization</h3>

Converting the sentences in text into individual units
(or) words is called tokenization.

<h3>Normalization</h3>

Normalization is a systematic approach of decomposing tables to eliminate data redundancy(repetition) and undesirable characteristics like Insertion, Update and Deletion Anomalies. It is a multi-step process that puts data into tabular form, removing duplicated data from the relation tables.

<h3>Stemming</h3>

Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP). ... Stemming is also a part of queries and Internet search engines.

<h3>Lemmatization</h3>

Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .

<h3>Feature extraction</h3>

Feature extraction step means to extract and produce feature representations that are appropriate for the type of NLP task you are trying to accomplish and the type of model you are planning to use.

<h3>Bag of words</h3>

Bag of Words model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words. ... This model can be visualized using a table, which contains the count of words corresponding to the word itself.

<h3>TF-IDF</h3>

TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.

It has many uses, most importantly in automated text analysis, and is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP).

TF-IDF (term frequency-inverse document frequency) was invented for document search and information retrieval. It works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don’t mean much to that document in particular.
